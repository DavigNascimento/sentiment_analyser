{
  "best_global_step": 22470,
  "best_metric": 0.9968264102935791,
  "best_model_checkpoint": "./results/checkpoint-22470",
  "epoch": 7.0,
  "eval_steps": 500,
  "global_step": 22470,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.01557632398753894,
      "grad_norm": 4.33573579788208,
      "learning_rate": 1.9955496217178463e-05,
      "loss": 3.1944,
      "step": 50
    },
    {
      "epoch": 0.03115264797507788,
      "grad_norm": 5.738800525665283,
      "learning_rate": 1.991099243435692e-05,
      "loss": 2.9875,
      "step": 100
    },
    {
      "epoch": 0.04672897196261682,
      "grad_norm": 7.371567726135254,
      "learning_rate": 1.9866488651535382e-05,
      "loss": 2.8008,
      "step": 150
    },
    {
      "epoch": 0.06230529595015576,
      "grad_norm": 5.086735248565674,
      "learning_rate": 1.982198486871384e-05,
      "loss": 2.7182,
      "step": 200
    },
    {
      "epoch": 0.0778816199376947,
      "grad_norm": 12.767690658569336,
      "learning_rate": 1.97774810858923e-05,
      "loss": 2.6351,
      "step": 250
    },
    {
      "epoch": 0.09345794392523364,
      "grad_norm": 7.975155830383301,
      "learning_rate": 1.9732977303070763e-05,
      "loss": 2.6519,
      "step": 300
    },
    {
      "epoch": 0.10903426791277258,
      "grad_norm": 8.135720252990723,
      "learning_rate": 1.9688473520249224e-05,
      "loss": 2.6085,
      "step": 350
    },
    {
      "epoch": 0.12461059190031153,
      "grad_norm": 5.993001461029053,
      "learning_rate": 1.9643969737427682e-05,
      "loss": 2.4483,
      "step": 400
    },
    {
      "epoch": 0.14018691588785046,
      "grad_norm": 6.475571155548096,
      "learning_rate": 1.9599465954606144e-05,
      "loss": 2.3928,
      "step": 450
    },
    {
      "epoch": 0.1557632398753894,
      "grad_norm": 8.414634704589844,
      "learning_rate": 1.9554962171784605e-05,
      "loss": 2.4336,
      "step": 500
    },
    {
      "epoch": 0.17133956386292834,
      "grad_norm": 6.429490566253662,
      "learning_rate": 1.9510458388963063e-05,
      "loss": 2.2634,
      "step": 550
    },
    {
      "epoch": 0.18691588785046728,
      "grad_norm": 11.786934852600098,
      "learning_rate": 1.9465954606141524e-05,
      "loss": 2.3543,
      "step": 600
    },
    {
      "epoch": 0.20249221183800623,
      "grad_norm": 7.339449882507324,
      "learning_rate": 1.9421450823319982e-05,
      "loss": 2.2204,
      "step": 650
    },
    {
      "epoch": 0.21806853582554517,
      "grad_norm": 10.245854377746582,
      "learning_rate": 1.9376947040498444e-05,
      "loss": 2.2104,
      "step": 700
    },
    {
      "epoch": 0.2336448598130841,
      "grad_norm": 9.718302726745605,
      "learning_rate": 1.9332443257676905e-05,
      "loss": 2.1577,
      "step": 750
    },
    {
      "epoch": 0.24922118380062305,
      "grad_norm": 10.649384498596191,
      "learning_rate": 1.9287939474855363e-05,
      "loss": 2.2157,
      "step": 800
    },
    {
      "epoch": 0.26479750778816197,
      "grad_norm": 12.442715644836426,
      "learning_rate": 1.9243435692033824e-05,
      "loss": 2.1845,
      "step": 850
    },
    {
      "epoch": 0.2803738317757009,
      "grad_norm": 7.5929083824157715,
      "learning_rate": 1.9198931909212282e-05,
      "loss": 2.1395,
      "step": 900
    },
    {
      "epoch": 0.29595015576323985,
      "grad_norm": 10.864206314086914,
      "learning_rate": 1.9154428126390744e-05,
      "loss": 2.05,
      "step": 950
    },
    {
      "epoch": 0.3115264797507788,
      "grad_norm": 8.313023567199707,
      "learning_rate": 1.9109924343569205e-05,
      "loss": 2.132,
      "step": 1000
    },
    {
      "epoch": 0.32710280373831774,
      "grad_norm": 12.81214427947998,
      "learning_rate": 1.9065420560747666e-05,
      "loss": 2.1321,
      "step": 1050
    },
    {
      "epoch": 0.3426791277258567,
      "grad_norm": 8.5027494430542,
      "learning_rate": 1.9020916777926128e-05,
      "loss": 2.1575,
      "step": 1100
    },
    {
      "epoch": 0.3582554517133956,
      "grad_norm": 9.993279457092285,
      "learning_rate": 1.8976412995104586e-05,
      "loss": 2.1121,
      "step": 1150
    },
    {
      "epoch": 0.37383177570093457,
      "grad_norm": 10.520489692687988,
      "learning_rate": 1.8931909212283047e-05,
      "loss": 2.108,
      "step": 1200
    },
    {
      "epoch": 0.3894080996884735,
      "grad_norm": 12.600570678710938,
      "learning_rate": 1.8887405429461505e-05,
      "loss": 2.0552,
      "step": 1250
    },
    {
      "epoch": 0.40498442367601245,
      "grad_norm": 11.720980644226074,
      "learning_rate": 1.8842901646639966e-05,
      "loss": 2.1127,
      "step": 1300
    },
    {
      "epoch": 0.4205607476635514,
      "grad_norm": 7.051231384277344,
      "learning_rate": 1.8798397863818424e-05,
      "loss": 1.9233,
      "step": 1350
    },
    {
      "epoch": 0.43613707165109034,
      "grad_norm": 10.147491455078125,
      "learning_rate": 1.8753894080996886e-05,
      "loss": 2.0689,
      "step": 1400
    },
    {
      "epoch": 0.4517133956386293,
      "grad_norm": 11.072951316833496,
      "learning_rate": 1.8709390298175347e-05,
      "loss": 1.9889,
      "step": 1450
    },
    {
      "epoch": 0.4672897196261682,
      "grad_norm": 10.072755813598633,
      "learning_rate": 1.8664886515353805e-05,
      "loss": 2.0564,
      "step": 1500
    },
    {
      "epoch": 0.48286604361370716,
      "grad_norm": 9.465967178344727,
      "learning_rate": 1.8620382732532266e-05,
      "loss": 1.974,
      "step": 1550
    },
    {
      "epoch": 0.4984423676012461,
      "grad_norm": 10.693336486816406,
      "learning_rate": 1.8575878949710728e-05,
      "loss": 1.9466,
      "step": 1600
    },
    {
      "epoch": 0.514018691588785,
      "grad_norm": 10.296083450317383,
      "learning_rate": 1.8531375166889186e-05,
      "loss": 2.0342,
      "step": 1650
    },
    {
      "epoch": 0.5295950155763239,
      "grad_norm": 9.89687442779541,
      "learning_rate": 1.8486871384067647e-05,
      "loss": 1.9396,
      "step": 1700
    },
    {
      "epoch": 0.5451713395638629,
      "grad_norm": 11.324433326721191,
      "learning_rate": 1.8442367601246108e-05,
      "loss": 1.967,
      "step": 1750
    },
    {
      "epoch": 0.5607476635514018,
      "grad_norm": 11.8146390914917,
      "learning_rate": 1.839786381842457e-05,
      "loss": 1.9385,
      "step": 1800
    },
    {
      "epoch": 0.5763239875389408,
      "grad_norm": 14.931709289550781,
      "learning_rate": 1.8353360035603028e-05,
      "loss": 1.8631,
      "step": 1850
    },
    {
      "epoch": 0.5919003115264797,
      "grad_norm": 12.39860725402832,
      "learning_rate": 1.830885625278149e-05,
      "loss": 1.8748,
      "step": 1900
    },
    {
      "epoch": 0.6074766355140186,
      "grad_norm": 10.636343002319336,
      "learning_rate": 1.8264352469959947e-05,
      "loss": 1.8863,
      "step": 1950
    },
    {
      "epoch": 0.6230529595015576,
      "grad_norm": 12.597943305969238,
      "learning_rate": 1.8219848687138408e-05,
      "loss": 1.8841,
      "step": 2000
    },
    {
      "epoch": 0.6386292834890965,
      "grad_norm": 15.444485664367676,
      "learning_rate": 1.817534490431687e-05,
      "loss": 1.9145,
      "step": 2050
    },
    {
      "epoch": 0.6542056074766355,
      "grad_norm": 10.052681922912598,
      "learning_rate": 1.8130841121495328e-05,
      "loss": 1.859,
      "step": 2100
    },
    {
      "epoch": 0.6697819314641744,
      "grad_norm": 14.722332954406738,
      "learning_rate": 1.808633733867379e-05,
      "loss": 1.8665,
      "step": 2150
    },
    {
      "epoch": 0.6853582554517134,
      "grad_norm": 12.077034950256348,
      "learning_rate": 1.8041833555852247e-05,
      "loss": 1.8505,
      "step": 2200
    },
    {
      "epoch": 0.7009345794392523,
      "grad_norm": 13.574102401733398,
      "learning_rate": 1.7997329773030708e-05,
      "loss": 1.9015,
      "step": 2250
    },
    {
      "epoch": 0.7165109034267912,
      "grad_norm": 9.972302436828613,
      "learning_rate": 1.795282599020917e-05,
      "loss": 1.7944,
      "step": 2300
    },
    {
      "epoch": 0.7320872274143302,
      "grad_norm": 15.21760082244873,
      "learning_rate": 1.790832220738763e-05,
      "loss": 1.8143,
      "step": 2350
    },
    {
      "epoch": 0.7476635514018691,
      "grad_norm": 14.787763595581055,
      "learning_rate": 1.786381842456609e-05,
      "loss": 1.8558,
      "step": 2400
    },
    {
      "epoch": 0.7632398753894081,
      "grad_norm": 9.510965347290039,
      "learning_rate": 1.781931464174455e-05,
      "loss": 1.7692,
      "step": 2450
    },
    {
      "epoch": 0.778816199376947,
      "grad_norm": 10.39967155456543,
      "learning_rate": 1.777481085892301e-05,
      "loss": 1.7312,
      "step": 2500
    },
    {
      "epoch": 0.794392523364486,
      "grad_norm": 12.265216827392578,
      "learning_rate": 1.773030707610147e-05,
      "loss": 1.8046,
      "step": 2550
    },
    {
      "epoch": 0.8099688473520249,
      "grad_norm": 14.338418006896973,
      "learning_rate": 1.768580329327993e-05,
      "loss": 1.7867,
      "step": 2600
    },
    {
      "epoch": 0.8255451713395638,
      "grad_norm": 11.399542808532715,
      "learning_rate": 1.764129951045839e-05,
      "loss": 1.8698,
      "step": 2650
    },
    {
      "epoch": 0.8411214953271028,
      "grad_norm": 10.439302444458008,
      "learning_rate": 1.759679572763685e-05,
      "loss": 1.7663,
      "step": 2700
    },
    {
      "epoch": 0.8566978193146417,
      "grad_norm": 10.009536743164062,
      "learning_rate": 1.755229194481531e-05,
      "loss": 1.8517,
      "step": 2750
    },
    {
      "epoch": 0.8722741433021807,
      "grad_norm": 15.514829635620117,
      "learning_rate": 1.750778816199377e-05,
      "loss": 1.7172,
      "step": 2800
    },
    {
      "epoch": 0.8878504672897196,
      "grad_norm": 11.537951469421387,
      "learning_rate": 1.746328437917223e-05,
      "loss": 1.7369,
      "step": 2850
    },
    {
      "epoch": 0.9034267912772586,
      "grad_norm": 12.080269813537598,
      "learning_rate": 1.7418780596350692e-05,
      "loss": 1.698,
      "step": 2900
    },
    {
      "epoch": 0.9190031152647975,
      "grad_norm": 9.211281776428223,
      "learning_rate": 1.737427681352915e-05,
      "loss": 1.6758,
      "step": 2950
    },
    {
      "epoch": 0.9345794392523364,
      "grad_norm": 10.26553726196289,
      "learning_rate": 1.732977303070761e-05,
      "loss": 1.6766,
      "step": 3000
    },
    {
      "epoch": 0.9501557632398754,
      "grad_norm": 11.646489143371582,
      "learning_rate": 1.7285269247886073e-05,
      "loss": 1.7023,
      "step": 3050
    },
    {
      "epoch": 0.9657320872274143,
      "grad_norm": 12.882267951965332,
      "learning_rate": 1.7240765465064534e-05,
      "loss": 1.6472,
      "step": 3100
    },
    {
      "epoch": 0.9813084112149533,
      "grad_norm": 8.493605613708496,
      "learning_rate": 1.7196261682242992e-05,
      "loss": 1.6513,
      "step": 3150
    },
    {
      "epoch": 0.9968847352024922,
      "grad_norm": 15.217893600463867,
      "learning_rate": 1.7151757899421454e-05,
      "loss": 1.6584,
      "step": 3200
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.5575889728214314,
      "eval_f1_score": 0.5480857442540114,
      "eval_loss": 1.6197415590286255,
      "eval_runtime": 45.7912,
      "eval_samples_per_second": 280.425,
      "eval_steps_per_second": 17.536,
      "step": 3210
    },
    {
      "epoch": 1.0124610591900312,
      "grad_norm": 12.60213565826416,
      "learning_rate": 1.710725411659991e-05,
      "loss": 1.6791,
      "step": 3250
    },
    {
      "epoch": 1.02803738317757,
      "grad_norm": 13.563323974609375,
      "learning_rate": 1.7062750333778373e-05,
      "loss": 1.5573,
      "step": 3300
    },
    {
      "epoch": 1.043613707165109,
      "grad_norm": 14.329927444458008,
      "learning_rate": 1.701824655095683e-05,
      "loss": 1.4588,
      "step": 3350
    },
    {
      "epoch": 1.0591900311526479,
      "grad_norm": 12.677983283996582,
      "learning_rate": 1.6973742768135292e-05,
      "loss": 1.5351,
      "step": 3400
    },
    {
      "epoch": 1.074766355140187,
      "grad_norm": 10.975924491882324,
      "learning_rate": 1.6929238985313754e-05,
      "loss": 1.4804,
      "step": 3450
    },
    {
      "epoch": 1.0903426791277258,
      "grad_norm": 15.90013313293457,
      "learning_rate": 1.688473520249221e-05,
      "loss": 1.4395,
      "step": 3500
    },
    {
      "epoch": 1.1059190031152648,
      "grad_norm": 13.618464469909668,
      "learning_rate": 1.6840231419670673e-05,
      "loss": 1.5087,
      "step": 3550
    },
    {
      "epoch": 1.1214953271028036,
      "grad_norm": 14.146931648254395,
      "learning_rate": 1.6795727636849134e-05,
      "loss": 1.432,
      "step": 3600
    },
    {
      "epoch": 1.1370716510903427,
      "grad_norm": 15.200118064880371,
      "learning_rate": 1.6751223854027596e-05,
      "loss": 1.4937,
      "step": 3650
    },
    {
      "epoch": 1.1526479750778815,
      "grad_norm": 14.481396675109863,
      "learning_rate": 1.6706720071206054e-05,
      "loss": 1.448,
      "step": 3700
    },
    {
      "epoch": 1.1682242990654206,
      "grad_norm": 11.03680419921875,
      "learning_rate": 1.6662216288384515e-05,
      "loss": 1.463,
      "step": 3750
    },
    {
      "epoch": 1.1838006230529594,
      "grad_norm": 12.331186294555664,
      "learning_rate": 1.6617712505562976e-05,
      "loss": 1.5182,
      "step": 3800
    },
    {
      "epoch": 1.1993769470404985,
      "grad_norm": 13.657190322875977,
      "learning_rate": 1.6573208722741434e-05,
      "loss": 1.5042,
      "step": 3850
    },
    {
      "epoch": 1.2149532710280373,
      "grad_norm": 12.51646614074707,
      "learning_rate": 1.6528704939919896e-05,
      "loss": 1.4973,
      "step": 3900
    },
    {
      "epoch": 1.2305295950155763,
      "grad_norm": 13.498745918273926,
      "learning_rate": 1.6484201157098354e-05,
      "loss": 1.574,
      "step": 3950
    },
    {
      "epoch": 1.2461059190031152,
      "grad_norm": 12.446544647216797,
      "learning_rate": 1.6439697374276815e-05,
      "loss": 1.3757,
      "step": 4000
    },
    {
      "epoch": 1.2616822429906542,
      "grad_norm": 12.92896842956543,
      "learning_rate": 1.6395193591455273e-05,
      "loss": 1.4162,
      "step": 4050
    },
    {
      "epoch": 1.277258566978193,
      "grad_norm": 14.414350509643555,
      "learning_rate": 1.6350689808633734e-05,
      "loss": 1.4426,
      "step": 4100
    },
    {
      "epoch": 1.2928348909657321,
      "grad_norm": 12.189388275146484,
      "learning_rate": 1.6306186025812196e-05,
      "loss": 1.4076,
      "step": 4150
    },
    {
      "epoch": 1.308411214953271,
      "grad_norm": 19.242660522460938,
      "learning_rate": 1.6261682242990654e-05,
      "loss": 1.4888,
      "step": 4200
    },
    {
      "epoch": 1.32398753894081,
      "grad_norm": 10.717280387878418,
      "learning_rate": 1.6217178460169115e-05,
      "loss": 1.4242,
      "step": 4250
    },
    {
      "epoch": 1.3395638629283488,
      "grad_norm": 16.728580474853516,
      "learning_rate": 1.6172674677347576e-05,
      "loss": 1.4132,
      "step": 4300
    },
    {
      "epoch": 1.355140186915888,
      "grad_norm": 16.3824520111084,
      "learning_rate": 1.6128170894526038e-05,
      "loss": 1.4296,
      "step": 4350
    },
    {
      "epoch": 1.3707165109034267,
      "grad_norm": 14.049299240112305,
      "learning_rate": 1.6083667111704496e-05,
      "loss": 1.4455,
      "step": 4400
    },
    {
      "epoch": 1.3862928348909658,
      "grad_norm": 14.981372833251953,
      "learning_rate": 1.6039163328882957e-05,
      "loss": 1.4173,
      "step": 4450
    },
    {
      "epoch": 1.4018691588785046,
      "grad_norm": 14.507749557495117,
      "learning_rate": 1.5994659546061418e-05,
      "loss": 1.4524,
      "step": 4500
    },
    {
      "epoch": 1.4174454828660437,
      "grad_norm": 11.993805885314941,
      "learning_rate": 1.5950155763239876e-05,
      "loss": 1.3611,
      "step": 4550
    },
    {
      "epoch": 1.4330218068535825,
      "grad_norm": 14.716119766235352,
      "learning_rate": 1.5905651980418338e-05,
      "loss": 1.5309,
      "step": 4600
    },
    {
      "epoch": 1.4485981308411215,
      "grad_norm": 13.926881790161133,
      "learning_rate": 1.5861148197596796e-05,
      "loss": 1.3487,
      "step": 4650
    },
    {
      "epoch": 1.4641744548286604,
      "grad_norm": 18.201580047607422,
      "learning_rate": 1.5816644414775257e-05,
      "loss": 1.4366,
      "step": 4700
    },
    {
      "epoch": 1.4797507788161994,
      "grad_norm": 17.145647048950195,
      "learning_rate": 1.5772140631953715e-05,
      "loss": 1.4037,
      "step": 4750
    },
    {
      "epoch": 1.4953271028037383,
      "grad_norm": 17.837888717651367,
      "learning_rate": 1.5727636849132176e-05,
      "loss": 1.3808,
      "step": 4800
    },
    {
      "epoch": 1.5109034267912773,
      "grad_norm": 16.142621994018555,
      "learning_rate": 1.5683133066310638e-05,
      "loss": 1.4029,
      "step": 4850
    },
    {
      "epoch": 1.5264797507788161,
      "grad_norm": 7.570511817932129,
      "learning_rate": 1.56386292834891e-05,
      "loss": 1.3791,
      "step": 4900
    },
    {
      "epoch": 1.542056074766355,
      "grad_norm": 13.406476020812988,
      "learning_rate": 1.5594125500667557e-05,
      "loss": 1.4278,
      "step": 4950
    },
    {
      "epoch": 1.557632398753894,
      "grad_norm": 11.97409725189209,
      "learning_rate": 1.5549621717846018e-05,
      "loss": 1.3958,
      "step": 5000
    },
    {
      "epoch": 1.573208722741433,
      "grad_norm": 18.748992919921875,
      "learning_rate": 1.550511793502448e-05,
      "loss": 1.3591,
      "step": 5050
    },
    {
      "epoch": 1.588785046728972,
      "grad_norm": 18.471843719482422,
      "learning_rate": 1.546061415220294e-05,
      "loss": 1.3441,
      "step": 5100
    },
    {
      "epoch": 1.6043613707165107,
      "grad_norm": 14.33719539642334,
      "learning_rate": 1.54161103693814e-05,
      "loss": 1.3557,
      "step": 5150
    },
    {
      "epoch": 1.6199376947040498,
      "grad_norm": 15.510575294494629,
      "learning_rate": 1.537160658655986e-05,
      "loss": 1.3342,
      "step": 5200
    },
    {
      "epoch": 1.6355140186915889,
      "grad_norm": 13.673930168151855,
      "learning_rate": 1.5327102803738318e-05,
      "loss": 1.4051,
      "step": 5250
    },
    {
      "epoch": 1.6510903426791277,
      "grad_norm": 20.540298461914062,
      "learning_rate": 1.528259902091678e-05,
      "loss": 1.3321,
      "step": 5300
    },
    {
      "epoch": 1.6666666666666665,
      "grad_norm": 12.283902168273926,
      "learning_rate": 1.523809523809524e-05,
      "loss": 1.2627,
      "step": 5350
    },
    {
      "epoch": 1.6822429906542056,
      "grad_norm": 17.018552780151367,
      "learning_rate": 1.5193591455273699e-05,
      "loss": 1.3227,
      "step": 5400
    },
    {
      "epoch": 1.6978193146417446,
      "grad_norm": 17.250337600708008,
      "learning_rate": 1.514908767245216e-05,
      "loss": 1.3842,
      "step": 5450
    },
    {
      "epoch": 1.7133956386292835,
      "grad_norm": 12.021384239196777,
      "learning_rate": 1.510458388963062e-05,
      "loss": 1.3613,
      "step": 5500
    },
    {
      "epoch": 1.7289719626168223,
      "grad_norm": 14.249425888061523,
      "learning_rate": 1.5060080106809081e-05,
      "loss": 1.3667,
      "step": 5550
    },
    {
      "epoch": 1.7445482866043613,
      "grad_norm": 15.098621368408203,
      "learning_rate": 1.501557632398754e-05,
      "loss": 1.384,
      "step": 5600
    },
    {
      "epoch": 1.7601246105919004,
      "grad_norm": 15.090240478515625,
      "learning_rate": 1.4971072541166e-05,
      "loss": 1.3566,
      "step": 5650
    },
    {
      "epoch": 1.7757009345794392,
      "grad_norm": 23.200206756591797,
      "learning_rate": 1.492656875834446e-05,
      "loss": 1.3989,
      "step": 5700
    },
    {
      "epoch": 1.791277258566978,
      "grad_norm": 14.082959175109863,
      "learning_rate": 1.488206497552292e-05,
      "loss": 1.338,
      "step": 5750
    },
    {
      "epoch": 1.8068535825545171,
      "grad_norm": 15.0516939163208,
      "learning_rate": 1.4837561192701381e-05,
      "loss": 1.371,
      "step": 5800
    },
    {
      "epoch": 1.8224299065420562,
      "grad_norm": 13.772356033325195,
      "learning_rate": 1.4793057409879841e-05,
      "loss": 1.2929,
      "step": 5850
    },
    {
      "epoch": 1.838006230529595,
      "grad_norm": 15.474552154541016,
      "learning_rate": 1.4748553627058302e-05,
      "loss": 1.3307,
      "step": 5900
    },
    {
      "epoch": 1.8535825545171338,
      "grad_norm": 15.392455101013184,
      "learning_rate": 1.470404984423676e-05,
      "loss": 1.324,
      "step": 5950
    },
    {
      "epoch": 1.8691588785046729,
      "grad_norm": 10.554064750671387,
      "learning_rate": 1.4659546061415222e-05,
      "loss": 1.3067,
      "step": 6000
    },
    {
      "epoch": 1.884735202492212,
      "grad_norm": 14.9478178024292,
      "learning_rate": 1.4615042278593681e-05,
      "loss": 1.3927,
      "step": 6050
    },
    {
      "epoch": 1.9003115264797508,
      "grad_norm": 15.03149127960205,
      "learning_rate": 1.4570538495772143e-05,
      "loss": 1.3199,
      "step": 6100
    },
    {
      "epoch": 1.9158878504672896,
      "grad_norm": 14.352166175842285,
      "learning_rate": 1.4526034712950602e-05,
      "loss": 1.2615,
      "step": 6150
    },
    {
      "epoch": 1.9314641744548287,
      "grad_norm": 20.106626510620117,
      "learning_rate": 1.4481530930129062e-05,
      "loss": 1.3749,
      "step": 6200
    },
    {
      "epoch": 1.9470404984423677,
      "grad_norm": 14.634481430053711,
      "learning_rate": 1.4437027147307523e-05,
      "loss": 1.3483,
      "step": 6250
    },
    {
      "epoch": 1.9626168224299065,
      "grad_norm": 21.134103775024414,
      "learning_rate": 1.4392523364485981e-05,
      "loss": 1.2503,
      "step": 6300
    },
    {
      "epoch": 1.9781931464174454,
      "grad_norm": 19.402414321899414,
      "learning_rate": 1.4348019581664443e-05,
      "loss": 1.3304,
      "step": 6350
    },
    {
      "epoch": 1.9937694704049844,
      "grad_norm": 16.513774871826172,
      "learning_rate": 1.4303515798842902e-05,
      "loss": 1.3031,
      "step": 6400
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.6290787321859668,
      "eval_f1_score": 0.6204954919948494,
      "eval_loss": 1.3475217819213867,
      "eval_runtime": 45.6717,
      "eval_samples_per_second": 281.159,
      "eval_steps_per_second": 17.582,
      "step": 6420
    },
    {
      "epoch": 2.0093457943925235,
      "grad_norm": 14.626394271850586,
      "learning_rate": 1.4259012016021364e-05,
      "loss": 1.1989,
      "step": 6450
    },
    {
      "epoch": 2.0249221183800623,
      "grad_norm": 11.681428909301758,
      "learning_rate": 1.4214508233199823e-05,
      "loss": 1.1538,
      "step": 6500
    },
    {
      "epoch": 2.040498442367601,
      "grad_norm": 15.269892692565918,
      "learning_rate": 1.4170004450378283e-05,
      "loss": 1.0255,
      "step": 6550
    },
    {
      "epoch": 2.05607476635514,
      "grad_norm": 19.55388832092285,
      "learning_rate": 1.4125500667556744e-05,
      "loss": 1.1055,
      "step": 6600
    },
    {
      "epoch": 2.0716510903426792,
      "grad_norm": 14.909717559814453,
      "learning_rate": 1.4080996884735202e-05,
      "loss": 1.082,
      "step": 6650
    },
    {
      "epoch": 2.087227414330218,
      "grad_norm": 15.527292251586914,
      "learning_rate": 1.4036493101913664e-05,
      "loss": 1.1281,
      "step": 6700
    },
    {
      "epoch": 2.102803738317757,
      "grad_norm": 16.817522048950195,
      "learning_rate": 1.3991989319092123e-05,
      "loss": 1.0662,
      "step": 6750
    },
    {
      "epoch": 2.1183800623052957,
      "grad_norm": 17.393369674682617,
      "learning_rate": 1.3947485536270585e-05,
      "loss": 1.0456,
      "step": 6800
    },
    {
      "epoch": 2.133956386292835,
      "grad_norm": 12.099794387817383,
      "learning_rate": 1.3902981753449046e-05,
      "loss": 1.0971,
      "step": 6850
    },
    {
      "epoch": 2.149532710280374,
      "grad_norm": 13.18266773223877,
      "learning_rate": 1.3858477970627504e-05,
      "loss": 1.0915,
      "step": 6900
    },
    {
      "epoch": 2.1651090342679127,
      "grad_norm": 12.366617202758789,
      "learning_rate": 1.3813974187805965e-05,
      "loss": 1.0387,
      "step": 6950
    },
    {
      "epoch": 2.1806853582554515,
      "grad_norm": 16.401935577392578,
      "learning_rate": 1.3769470404984425e-05,
      "loss": 1.0987,
      "step": 7000
    },
    {
      "epoch": 2.196261682242991,
      "grad_norm": 18.20452880859375,
      "learning_rate": 1.3724966622162885e-05,
      "loss": 1.1785,
      "step": 7050
    },
    {
      "epoch": 2.2118380062305296,
      "grad_norm": 11.201847076416016,
      "learning_rate": 1.3680462839341344e-05,
      "loss": 1.0961,
      "step": 7100
    },
    {
      "epoch": 2.2274143302180685,
      "grad_norm": 18.034521102905273,
      "learning_rate": 1.3635959056519806e-05,
      "loss": 1.1174,
      "step": 7150
    },
    {
      "epoch": 2.2429906542056073,
      "grad_norm": 13.25951099395752,
      "learning_rate": 1.3591455273698267e-05,
      "loss": 1.1025,
      "step": 7200
    },
    {
      "epoch": 2.2585669781931466,
      "grad_norm": 17.626075744628906,
      "learning_rate": 1.3546951490876725e-05,
      "loss": 1.1046,
      "step": 7250
    },
    {
      "epoch": 2.2741433021806854,
      "grad_norm": 18.9439640045166,
      "learning_rate": 1.3502447708055186e-05,
      "loss": 1.037,
      "step": 7300
    },
    {
      "epoch": 2.289719626168224,
      "grad_norm": 18.62969970703125,
      "learning_rate": 1.3457943925233646e-05,
      "loss": 1.1066,
      "step": 7350
    },
    {
      "epoch": 2.305295950155763,
      "grad_norm": 17.000520706176758,
      "learning_rate": 1.3413440142412106e-05,
      "loss": 1.1083,
      "step": 7400
    },
    {
      "epoch": 2.3208722741433023,
      "grad_norm": 18.018537521362305,
      "learning_rate": 1.3368936359590565e-05,
      "loss": 1.0969,
      "step": 7450
    },
    {
      "epoch": 2.336448598130841,
      "grad_norm": 19.75145721435547,
      "learning_rate": 1.3324432576769027e-05,
      "loss": 1.0759,
      "step": 7500
    },
    {
      "epoch": 2.35202492211838,
      "grad_norm": 13.293917655944824,
      "learning_rate": 1.3279928793947488e-05,
      "loss": 1.0912,
      "step": 7550
    },
    {
      "epoch": 2.367601246105919,
      "grad_norm": 17.7197265625,
      "learning_rate": 1.3235425011125946e-05,
      "loss": 1.1549,
      "step": 7600
    },
    {
      "epoch": 2.383177570093458,
      "grad_norm": 14.468088150024414,
      "learning_rate": 1.3190921228304407e-05,
      "loss": 0.9862,
      "step": 7650
    },
    {
      "epoch": 2.398753894080997,
      "grad_norm": 18.617347717285156,
      "learning_rate": 1.3146417445482867e-05,
      "loss": 1.0484,
      "step": 7700
    },
    {
      "epoch": 2.4143302180685358,
      "grad_norm": 13.261658668518066,
      "learning_rate": 1.3101913662661328e-05,
      "loss": 1.0767,
      "step": 7750
    },
    {
      "epoch": 2.4299065420560746,
      "grad_norm": 18.452125549316406,
      "learning_rate": 1.3057409879839786e-05,
      "loss": 1.0487,
      "step": 7800
    },
    {
      "epoch": 2.445482866043614,
      "grad_norm": 21.114330291748047,
      "learning_rate": 1.3012906097018248e-05,
      "loss": 1.0604,
      "step": 7850
    },
    {
      "epoch": 2.4610591900311527,
      "grad_norm": 12.985926628112793,
      "learning_rate": 1.2968402314196709e-05,
      "loss": 1.1211,
      "step": 7900
    },
    {
      "epoch": 2.4766355140186915,
      "grad_norm": 18.203977584838867,
      "learning_rate": 1.2923898531375167e-05,
      "loss": 1.1233,
      "step": 7950
    },
    {
      "epoch": 2.4922118380062304,
      "grad_norm": 19.975982666015625,
      "learning_rate": 1.2879394748553628e-05,
      "loss": 1.0558,
      "step": 8000
    },
    {
      "epoch": 2.507788161993769,
      "grad_norm": 17.426450729370117,
      "learning_rate": 1.2834890965732088e-05,
      "loss": 1.1291,
      "step": 8050
    },
    {
      "epoch": 2.5233644859813085,
      "grad_norm": 25.309480667114258,
      "learning_rate": 1.279038718291055e-05,
      "loss": 1.1189,
      "step": 8100
    },
    {
      "epoch": 2.5389408099688473,
      "grad_norm": 19.448755264282227,
      "learning_rate": 1.2745883400089009e-05,
      "loss": 1.0773,
      "step": 8150
    },
    {
      "epoch": 2.554517133956386,
      "grad_norm": 26.69015121459961,
      "learning_rate": 1.2701379617267469e-05,
      "loss": 1.0888,
      "step": 8200
    },
    {
      "epoch": 2.5700934579439254,
      "grad_norm": 18.36266326904297,
      "learning_rate": 1.265687583444593e-05,
      "loss": 1.1022,
      "step": 8250
    },
    {
      "epoch": 2.5856697819314642,
      "grad_norm": 18.49786949157715,
      "learning_rate": 1.2612372051624388e-05,
      "loss": 0.9295,
      "step": 8300
    },
    {
      "epoch": 2.601246105919003,
      "grad_norm": 20.047073364257812,
      "learning_rate": 1.256786826880285e-05,
      "loss": 1.0568,
      "step": 8350
    },
    {
      "epoch": 2.616822429906542,
      "grad_norm": 19.54388427734375,
      "learning_rate": 1.2523364485981309e-05,
      "loss": 1.0619,
      "step": 8400
    },
    {
      "epoch": 2.6323987538940807,
      "grad_norm": 20.639680862426758,
      "learning_rate": 1.247886070315977e-05,
      "loss": 1.0506,
      "step": 8450
    },
    {
      "epoch": 2.64797507788162,
      "grad_norm": 28.39208221435547,
      "learning_rate": 1.2434356920338232e-05,
      "loss": 1.1347,
      "step": 8500
    },
    {
      "epoch": 2.663551401869159,
      "grad_norm": 15.099830627441406,
      "learning_rate": 1.238985313751669e-05,
      "loss": 1.0464,
      "step": 8550
    },
    {
      "epoch": 2.6791277258566977,
      "grad_norm": 14.038163185119629,
      "learning_rate": 1.2345349354695151e-05,
      "loss": 1.124,
      "step": 8600
    },
    {
      "epoch": 2.694704049844237,
      "grad_norm": 17.034446716308594,
      "learning_rate": 1.2300845571873609e-05,
      "loss": 1.0557,
      "step": 8650
    },
    {
      "epoch": 2.710280373831776,
      "grad_norm": 25.736804962158203,
      "learning_rate": 1.225634178905207e-05,
      "loss": 0.994,
      "step": 8700
    },
    {
      "epoch": 2.7258566978193146,
      "grad_norm": 16.185815811157227,
      "learning_rate": 1.221183800623053e-05,
      "loss": 1.1313,
      "step": 8750
    },
    {
      "epoch": 2.7414330218068534,
      "grad_norm": 21.707120895385742,
      "learning_rate": 1.2167334223408991e-05,
      "loss": 0.9802,
      "step": 8800
    },
    {
      "epoch": 2.7570093457943923,
      "grad_norm": 19.05549430847168,
      "learning_rate": 1.2122830440587453e-05,
      "loss": 1.1102,
      "step": 8850
    },
    {
      "epoch": 2.7725856697819315,
      "grad_norm": 14.123106002807617,
      "learning_rate": 1.207832665776591e-05,
      "loss": 0.9756,
      "step": 8900
    },
    {
      "epoch": 2.7881619937694704,
      "grad_norm": 17.63027000427246,
      "learning_rate": 1.2033822874944372e-05,
      "loss": 1.0306,
      "step": 8950
    },
    {
      "epoch": 2.803738317757009,
      "grad_norm": 20.33976173400879,
      "learning_rate": 1.1989319092122832e-05,
      "loss": 0.9829,
      "step": 9000
    },
    {
      "epoch": 2.8193146417445485,
      "grad_norm": 24.19092559814453,
      "learning_rate": 1.1944815309301291e-05,
      "loss": 1.0431,
      "step": 9050
    },
    {
      "epoch": 2.8348909657320873,
      "grad_norm": 15.947429656982422,
      "learning_rate": 1.1900311526479751e-05,
      "loss": 1.0272,
      "step": 9100
    },
    {
      "epoch": 2.850467289719626,
      "grad_norm": 18.281837463378906,
      "learning_rate": 1.1855807743658212e-05,
      "loss": 0.9828,
      "step": 9150
    },
    {
      "epoch": 2.866043613707165,
      "grad_norm": 22.097244262695312,
      "learning_rate": 1.1811303960836674e-05,
      "loss": 1.0269,
      "step": 9200
    },
    {
      "epoch": 2.881619937694704,
      "grad_norm": 16.52707862854004,
      "learning_rate": 1.1766800178015132e-05,
      "loss": 0.9684,
      "step": 9250
    },
    {
      "epoch": 2.897196261682243,
      "grad_norm": 11.426959991455078,
      "learning_rate": 1.1722296395193593e-05,
      "loss": 1.0372,
      "step": 9300
    },
    {
      "epoch": 2.912772585669782,
      "grad_norm": 13.572583198547363,
      "learning_rate": 1.1677792612372053e-05,
      "loss": 0.9534,
      "step": 9350
    },
    {
      "epoch": 2.9283489096573208,
      "grad_norm": 24.309280395507812,
      "learning_rate": 1.1633288829550514e-05,
      "loss": 1.0369,
      "step": 9400
    },
    {
      "epoch": 2.94392523364486,
      "grad_norm": 17.45936393737793,
      "learning_rate": 1.1588785046728972e-05,
      "loss": 1.0361,
      "step": 9450
    },
    {
      "epoch": 2.959501557632399,
      "grad_norm": 35.01875686645508,
      "learning_rate": 1.1544281263907433e-05,
      "loss": 0.9348,
      "step": 9500
    },
    {
      "epoch": 2.9750778816199377,
      "grad_norm": 20.759851455688477,
      "learning_rate": 1.1499777481085895e-05,
      "loss": 0.9602,
      "step": 9550
    },
    {
      "epoch": 2.9906542056074765,
      "grad_norm": 18.234983444213867,
      "learning_rate": 1.1455273698264353e-05,
      "loss": 1.1355,
      "step": 9600
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.6854606339070166,
      "eval_f1_score": 0.681813319701612,
      "eval_loss": 1.1758776903152466,
      "eval_runtime": 45.7223,
      "eval_samples_per_second": 280.847,
      "eval_steps_per_second": 17.563,
      "step": 9630
    },
    {
      "epoch": 3.0062305295950154,
      "grad_norm": 13.251405715942383,
      "learning_rate": 1.1410769915442814e-05,
      "loss": 0.8787,
      "step": 9650
    },
    {
      "epoch": 3.0218068535825546,
      "grad_norm": 28.201520919799805,
      "learning_rate": 1.1366266132621274e-05,
      "loss": 0.8729,
      "step": 9700
    },
    {
      "epoch": 3.0373831775700935,
      "grad_norm": 15.182821273803711,
      "learning_rate": 1.1321762349799735e-05,
      "loss": 0.8892,
      "step": 9750
    },
    {
      "epoch": 3.0529595015576323,
      "grad_norm": 16.851390838623047,
      "learning_rate": 1.1277258566978193e-05,
      "loss": 0.79,
      "step": 9800
    },
    {
      "epoch": 3.068535825545171,
      "grad_norm": 10.255760192871094,
      "learning_rate": 1.1232754784156654e-05,
      "loss": 0.8352,
      "step": 9850
    },
    {
      "epoch": 3.0841121495327104,
      "grad_norm": 22.361167907714844,
      "learning_rate": 1.1188251001335116e-05,
      "loss": 0.874,
      "step": 9900
    },
    {
      "epoch": 3.0996884735202492,
      "grad_norm": 16.386552810668945,
      "learning_rate": 1.1143747218513574e-05,
      "loss": 0.7487,
      "step": 9950
    },
    {
      "epoch": 3.115264797507788,
      "grad_norm": 14.418497085571289,
      "learning_rate": 1.1099243435692035e-05,
      "loss": 0.8356,
      "step": 10000
    },
    {
      "epoch": 3.130841121495327,
      "grad_norm": 13.680215835571289,
      "learning_rate": 1.1054739652870495e-05,
      "loss": 0.8058,
      "step": 10050
    },
    {
      "epoch": 3.146417445482866,
      "grad_norm": 11.057150840759277,
      "learning_rate": 1.1010235870048956e-05,
      "loss": 0.8079,
      "step": 10100
    },
    {
      "epoch": 3.161993769470405,
      "grad_norm": 23.83352279663086,
      "learning_rate": 1.0965732087227414e-05,
      "loss": 0.872,
      "step": 10150
    },
    {
      "epoch": 3.177570093457944,
      "grad_norm": 16.754011154174805,
      "learning_rate": 1.0921228304405875e-05,
      "loss": 0.9367,
      "step": 10200
    },
    {
      "epoch": 3.1931464174454827,
      "grad_norm": 20.330455780029297,
      "learning_rate": 1.0876724521584337e-05,
      "loss": 0.9346,
      "step": 10250
    },
    {
      "epoch": 3.208722741433022,
      "grad_norm": 11.036545753479004,
      "learning_rate": 1.0832220738762795e-05,
      "loss": 0.7991,
      "step": 10300
    },
    {
      "epoch": 3.2242990654205608,
      "grad_norm": 16.46639060974121,
      "learning_rate": 1.0787716955941256e-05,
      "loss": 0.8533,
      "step": 10350
    },
    {
      "epoch": 3.2398753894080996,
      "grad_norm": 18.79289436340332,
      "learning_rate": 1.0743213173119716e-05,
      "loss": 0.8792,
      "step": 10400
    },
    {
      "epoch": 3.2554517133956384,
      "grad_norm": 25.58113670349121,
      "learning_rate": 1.0698709390298177e-05,
      "loss": 0.784,
      "step": 10450
    },
    {
      "epoch": 3.2710280373831777,
      "grad_norm": 21.4886474609375,
      "learning_rate": 1.0654205607476635e-05,
      "loss": 0.8337,
      "step": 10500
    },
    {
      "epoch": 3.2866043613707165,
      "grad_norm": 23.728290557861328,
      "learning_rate": 1.0609701824655096e-05,
      "loss": 0.8672,
      "step": 10550
    },
    {
      "epoch": 3.3021806853582554,
      "grad_norm": 13.720867156982422,
      "learning_rate": 1.0565198041833558e-05,
      "loss": 0.7661,
      "step": 10600
    },
    {
      "epoch": 3.317757009345794,
      "grad_norm": 17.2362060546875,
      "learning_rate": 1.0520694259012017e-05,
      "loss": 0.794,
      "step": 10650
    },
    {
      "epoch": 3.3333333333333335,
      "grad_norm": 17.964338302612305,
      "learning_rate": 1.0476190476190477e-05,
      "loss": 0.8864,
      "step": 10700
    },
    {
      "epoch": 3.3489096573208723,
      "grad_norm": 18.981969833374023,
      "learning_rate": 1.0431686693368937e-05,
      "loss": 0.8484,
      "step": 10750
    },
    {
      "epoch": 3.364485981308411,
      "grad_norm": 29.294437408447266,
      "learning_rate": 1.0387182910547398e-05,
      "loss": 0.7934,
      "step": 10800
    },
    {
      "epoch": 3.38006230529595,
      "grad_norm": 14.9331636428833,
      "learning_rate": 1.0342679127725856e-05,
      "loss": 0.8477,
      "step": 10850
    },
    {
      "epoch": 3.3956386292834893,
      "grad_norm": 21.787416458129883,
      "learning_rate": 1.0298175344904317e-05,
      "loss": 0.8271,
      "step": 10900
    },
    {
      "epoch": 3.411214953271028,
      "grad_norm": 13.301198959350586,
      "learning_rate": 1.0253671562082779e-05,
      "loss": 0.8481,
      "step": 10950
    },
    {
      "epoch": 3.426791277258567,
      "grad_norm": 23.360036849975586,
      "learning_rate": 1.0209167779261238e-05,
      "loss": 0.7279,
      "step": 11000
    },
    {
      "epoch": 3.4423676012461057,
      "grad_norm": 16.617673873901367,
      "learning_rate": 1.0164663996439698e-05,
      "loss": 0.7828,
      "step": 11050
    },
    {
      "epoch": 3.457943925233645,
      "grad_norm": 17.006486892700195,
      "learning_rate": 1.0120160213618158e-05,
      "loss": 0.8684,
      "step": 11100
    },
    {
      "epoch": 3.473520249221184,
      "grad_norm": 20.896835327148438,
      "learning_rate": 1.0075656430796619e-05,
      "loss": 0.7481,
      "step": 11150
    },
    {
      "epoch": 3.4890965732087227,
      "grad_norm": 15.496871948242188,
      "learning_rate": 1.0031152647975077e-05,
      "loss": 0.7574,
      "step": 11200
    },
    {
      "epoch": 3.5046728971962615,
      "grad_norm": 14.860738754272461,
      "learning_rate": 9.986648865153538e-06,
      "loss": 0.7992,
      "step": 11250
    },
    {
      "epoch": 3.520249221183801,
      "grad_norm": 18.923295974731445,
      "learning_rate": 9.942145082331998e-06,
      "loss": 0.8718,
      "step": 11300
    },
    {
      "epoch": 3.5358255451713396,
      "grad_norm": 23.3182430267334,
      "learning_rate": 9.89764129951046e-06,
      "loss": 0.8453,
      "step": 11350
    },
    {
      "epoch": 3.5514018691588785,
      "grad_norm": 26.543014526367188,
      "learning_rate": 9.85313751668892e-06,
      "loss": 0.7963,
      "step": 11400
    },
    {
      "epoch": 3.5669781931464173,
      "grad_norm": 19.546119689941406,
      "learning_rate": 9.80863373386738e-06,
      "loss": 0.8301,
      "step": 11450
    },
    {
      "epoch": 3.582554517133956,
      "grad_norm": 18.98798179626465,
      "learning_rate": 9.76412995104584e-06,
      "loss": 0.7615,
      "step": 11500
    },
    {
      "epoch": 3.5981308411214954,
      "grad_norm": 27.253559112548828,
      "learning_rate": 9.7196261682243e-06,
      "loss": 0.8647,
      "step": 11550
    },
    {
      "epoch": 3.6137071651090342,
      "grad_norm": 24.546913146972656,
      "learning_rate": 9.67512238540276e-06,
      "loss": 0.792,
      "step": 11600
    },
    {
      "epoch": 3.629283489096573,
      "grad_norm": 12.7645902633667,
      "learning_rate": 9.63061860258122e-06,
      "loss": 0.8019,
      "step": 11650
    },
    {
      "epoch": 3.6448598130841123,
      "grad_norm": 16.257888793945312,
      "learning_rate": 9.58611481975968e-06,
      "loss": 0.7401,
      "step": 11700
    },
    {
      "epoch": 3.660436137071651,
      "grad_norm": 18.448890686035156,
      "learning_rate": 9.541611036938142e-06,
      "loss": 0.8789,
      "step": 11750
    },
    {
      "epoch": 3.67601246105919,
      "grad_norm": 22.6422061920166,
      "learning_rate": 9.497107254116601e-06,
      "loss": 0.8031,
      "step": 11800
    },
    {
      "epoch": 3.691588785046729,
      "grad_norm": 21.530332565307617,
      "learning_rate": 9.452603471295061e-06,
      "loss": 0.6959,
      "step": 11850
    },
    {
      "epoch": 3.7071651090342677,
      "grad_norm": 20.381559371948242,
      "learning_rate": 9.40809968847352e-06,
      "loss": 0.7743,
      "step": 11900
    },
    {
      "epoch": 3.722741433021807,
      "grad_norm": 18.451377868652344,
      "learning_rate": 9.36359590565198e-06,
      "loss": 0.7685,
      "step": 11950
    },
    {
      "epoch": 3.7383177570093458,
      "grad_norm": 17.4648380279541,
      "learning_rate": 9.319092122830442e-06,
      "loss": 0.7672,
      "step": 12000
    },
    {
      "epoch": 3.7538940809968846,
      "grad_norm": 23.35550308227539,
      "learning_rate": 9.274588340008903e-06,
      "loss": 0.7987,
      "step": 12050
    },
    {
      "epoch": 3.769470404984424,
      "grad_norm": 17.783123016357422,
      "learning_rate": 9.230084557187363e-06,
      "loss": 0.7821,
      "step": 12100
    },
    {
      "epoch": 3.7850467289719627,
      "grad_norm": 13.76760482788086,
      "learning_rate": 9.185580774365822e-06,
      "loss": 0.8763,
      "step": 12150
    },
    {
      "epoch": 3.8006230529595015,
      "grad_norm": 17.74390411376953,
      "learning_rate": 9.141076991544282e-06,
      "loss": 0.873,
      "step": 12200
    },
    {
      "epoch": 3.8161993769470404,
      "grad_norm": 26.735042572021484,
      "learning_rate": 9.096573208722742e-06,
      "loss": 0.7528,
      "step": 12250
    },
    {
      "epoch": 3.831775700934579,
      "grad_norm": 17.970483779907227,
      "learning_rate": 9.052069425901203e-06,
      "loss": 0.8651,
      "step": 12300
    },
    {
      "epoch": 3.8473520249221185,
      "grad_norm": 27.257314682006836,
      "learning_rate": 9.007565643079663e-06,
      "loss": 0.7431,
      "step": 12350
    },
    {
      "epoch": 3.8629283489096573,
      "grad_norm": 15.297721862792969,
      "learning_rate": 8.963061860258124e-06,
      "loss": 0.8376,
      "step": 12400
    },
    {
      "epoch": 3.878504672897196,
      "grad_norm": 19.343271255493164,
      "learning_rate": 8.918558077436584e-06,
      "loss": 0.8853,
      "step": 12450
    },
    {
      "epoch": 3.8940809968847354,
      "grad_norm": 21.814165115356445,
      "learning_rate": 8.874054294615043e-06,
      "loss": 0.7566,
      "step": 12500
    },
    {
      "epoch": 3.9096573208722742,
      "grad_norm": 19.400611877441406,
      "learning_rate": 8.829550511793503e-06,
      "loss": 0.7683,
      "step": 12550
    },
    {
      "epoch": 3.925233644859813,
      "grad_norm": 27.139253616333008,
      "learning_rate": 8.785046728971963e-06,
      "loss": 0.817,
      "step": 12600
    },
    {
      "epoch": 3.940809968847352,
      "grad_norm": 34.005409240722656,
      "learning_rate": 8.740542946150424e-06,
      "loss": 0.8249,
      "step": 12650
    },
    {
      "epoch": 3.9563862928348907,
      "grad_norm": 13.79883861541748,
      "learning_rate": 8.696039163328884e-06,
      "loss": 0.7986,
      "step": 12700
    },
    {
      "epoch": 3.97196261682243,
      "grad_norm": 20.39197540283203,
      "learning_rate": 8.651535380507345e-06,
      "loss": 0.8081,
      "step": 12750
    },
    {
      "epoch": 3.987538940809969,
      "grad_norm": 41.175193786621094,
      "learning_rate": 8.607031597685805e-06,
      "loss": 0.7877,
      "step": 12800
    },
    {
      "epoch": 4.0,
      "eval_accuracy": 0.7233081535705942,
      "eval_f1_score": 0.7200085002200481,
      "eval_loss": 1.0977964401245117,
      "eval_runtime": 45.8007,
      "eval_samples_per_second": 280.367,
      "eval_steps_per_second": 17.532,
      "step": 12840
    },
    {
      "epoch": 4.003115264797508,
      "grad_norm": 21.602296829223633,
      "learning_rate": 8.562527814864264e-06,
      "loss": 0.8312,
      "step": 12850
    },
    {
      "epoch": 4.018691588785047,
      "grad_norm": 20.28590202331543,
      "learning_rate": 8.518024032042724e-06,
      "loss": 0.6753,
      "step": 12900
    },
    {
      "epoch": 4.034267912772585,
      "grad_norm": 22.615432739257812,
      "learning_rate": 8.473520249221184e-06,
      "loss": 0.6673,
      "step": 12950
    },
    {
      "epoch": 4.049844236760125,
      "grad_norm": 26.635597229003906,
      "learning_rate": 8.429016466399645e-06,
      "loss": 0.6189,
      "step": 13000
    },
    {
      "epoch": 4.065420560747664,
      "grad_norm": 30.705598831176758,
      "learning_rate": 8.384512683578105e-06,
      "loss": 0.6598,
      "step": 13050
    },
    {
      "epoch": 4.080996884735202,
      "grad_norm": 17.88168716430664,
      "learning_rate": 8.340008900756566e-06,
      "loss": 0.5914,
      "step": 13100
    },
    {
      "epoch": 4.0965732087227416,
      "grad_norm": 5.072402477264404,
      "learning_rate": 8.295505117935026e-06,
      "loss": 0.6468,
      "step": 13150
    },
    {
      "epoch": 4.11214953271028,
      "grad_norm": 22.89398193359375,
      "learning_rate": 8.251001335113485e-06,
      "loss": 0.7017,
      "step": 13200
    },
    {
      "epoch": 4.127725856697819,
      "grad_norm": 14.827219009399414,
      "learning_rate": 8.206497552291945e-06,
      "loss": 0.6792,
      "step": 13250
    },
    {
      "epoch": 4.1433021806853585,
      "grad_norm": 17.53773307800293,
      "learning_rate": 8.161993769470406e-06,
      "loss": 0.605,
      "step": 13300
    },
    {
      "epoch": 4.158878504672897,
      "grad_norm": 19.149913787841797,
      "learning_rate": 8.117489986648866e-06,
      "loss": 0.7096,
      "step": 13350
    },
    {
      "epoch": 4.174454828660436,
      "grad_norm": 13.273180961608887,
      "learning_rate": 8.072986203827326e-06,
      "loss": 0.5935,
      "step": 13400
    },
    {
      "epoch": 4.190031152647975,
      "grad_norm": 23.547222137451172,
      "learning_rate": 8.028482421005787e-06,
      "loss": 0.5716,
      "step": 13450
    },
    {
      "epoch": 4.205607476635514,
      "grad_norm": 18.419710159301758,
      "learning_rate": 7.983978638184247e-06,
      "loss": 0.5606,
      "step": 13500
    },
    {
      "epoch": 4.221183800623053,
      "grad_norm": 20.050094604492188,
      "learning_rate": 7.939474855362706e-06,
      "loss": 0.708,
      "step": 13550
    },
    {
      "epoch": 4.2367601246105915,
      "grad_norm": 18.231657028198242,
      "learning_rate": 7.894971072541166e-06,
      "loss": 0.5963,
      "step": 13600
    },
    {
      "epoch": 4.252336448598131,
      "grad_norm": 30.120391845703125,
      "learning_rate": 7.850467289719627e-06,
      "loss": 0.6894,
      "step": 13650
    },
    {
      "epoch": 4.26791277258567,
      "grad_norm": 18.9281005859375,
      "learning_rate": 7.805963506898087e-06,
      "loss": 0.6183,
      "step": 13700
    },
    {
      "epoch": 4.283489096573208,
      "grad_norm": 22.929786682128906,
      "learning_rate": 7.761459724076548e-06,
      "loss": 0.639,
      "step": 13750
    },
    {
      "epoch": 4.299065420560748,
      "grad_norm": 18.3945255279541,
      "learning_rate": 7.716955941255008e-06,
      "loss": 0.6462,
      "step": 13800
    },
    {
      "epoch": 4.314641744548287,
      "grad_norm": 20.485374450683594,
      "learning_rate": 7.672452158433468e-06,
      "loss": 0.611,
      "step": 13850
    },
    {
      "epoch": 4.330218068535825,
      "grad_norm": 22.34241485595703,
      "learning_rate": 7.627948375611928e-06,
      "loss": 0.6319,
      "step": 13900
    },
    {
      "epoch": 4.345794392523365,
      "grad_norm": 18.32533073425293,
      "learning_rate": 7.583444592790388e-06,
      "loss": 0.6532,
      "step": 13950
    },
    {
      "epoch": 4.361370716510903,
      "grad_norm": 27.85073471069336,
      "learning_rate": 7.538940809968847e-06,
      "loss": 0.7088,
      "step": 14000
    },
    {
      "epoch": 4.376947040498442,
      "grad_norm": 18.526264190673828,
      "learning_rate": 7.494437027147308e-06,
      "loss": 0.6422,
      "step": 14050
    },
    {
      "epoch": 4.392523364485982,
      "grad_norm": 21.8512020111084,
      "learning_rate": 7.449933244325769e-06,
      "loss": 0.6255,
      "step": 14100
    },
    {
      "epoch": 4.40809968847352,
      "grad_norm": 23.636600494384766,
      "learning_rate": 7.405429461504229e-06,
      "loss": 0.6637,
      "step": 14150
    },
    {
      "epoch": 4.423676012461059,
      "grad_norm": 21.322391510009766,
      "learning_rate": 7.3609256786826885e-06,
      "loss": 0.6079,
      "step": 14200
    },
    {
      "epoch": 4.4392523364485985,
      "grad_norm": 20.307758331298828,
      "learning_rate": 7.316421895861149e-06,
      "loss": 0.6519,
      "step": 14250
    },
    {
      "epoch": 4.454828660436137,
      "grad_norm": 28.288328170776367,
      "learning_rate": 7.271918113039609e-06,
      "loss": 0.6254,
      "step": 14300
    },
    {
      "epoch": 4.470404984423676,
      "grad_norm": 16.062969207763672,
      "learning_rate": 7.227414330218069e-06,
      "loss": 0.6838,
      "step": 14350
    },
    {
      "epoch": 4.485981308411215,
      "grad_norm": 15.833155632019043,
      "learning_rate": 7.182910547396529e-06,
      "loss": 0.7376,
      "step": 14400
    },
    {
      "epoch": 4.501557632398754,
      "grad_norm": 13.366423606872559,
      "learning_rate": 7.13840676457499e-06,
      "loss": 0.6146,
      "step": 14450
    },
    {
      "epoch": 4.517133956386293,
      "grad_norm": 26.50457763671875,
      "learning_rate": 7.09390298175345e-06,
      "loss": 0.6385,
      "step": 14500
    },
    {
      "epoch": 4.5327102803738315,
      "grad_norm": 20.47593879699707,
      "learning_rate": 7.0493991989319095e-06,
      "loss": 0.6955,
      "step": 14550
    },
    {
      "epoch": 4.548286604361371,
      "grad_norm": 20.712278366088867,
      "learning_rate": 7.00489541611037e-06,
      "loss": 0.6724,
      "step": 14600
    },
    {
      "epoch": 4.563862928348909,
      "grad_norm": 19.862215042114258,
      "learning_rate": 6.96039163328883e-06,
      "loss": 0.6412,
      "step": 14650
    },
    {
      "epoch": 4.579439252336448,
      "grad_norm": 30.609569549560547,
      "learning_rate": 6.91588785046729e-06,
      "loss": 0.6785,
      "step": 14700
    },
    {
      "epoch": 4.595015576323988,
      "grad_norm": 18.173582077026367,
      "learning_rate": 6.87138406764575e-06,
      "loss": 0.6881,
      "step": 14750
    },
    {
      "epoch": 4.610591900311526,
      "grad_norm": 22.93914222717285,
      "learning_rate": 6.826880284824211e-06,
      "loss": 0.6305,
      "step": 14800
    },
    {
      "epoch": 4.626168224299065,
      "grad_norm": 15.465299606323242,
      "learning_rate": 6.782376502002671e-06,
      "loss": 0.5409,
      "step": 14850
    },
    {
      "epoch": 4.641744548286605,
      "grad_norm": 25.114604949951172,
      "learning_rate": 6.737872719181131e-06,
      "loss": 0.6813,
      "step": 14900
    },
    {
      "epoch": 4.657320872274143,
      "grad_norm": 16.828659057617188,
      "learning_rate": 6.693368936359591e-06,
      "loss": 0.6457,
      "step": 14950
    },
    {
      "epoch": 4.672897196261682,
      "grad_norm": 16.8625545501709,
      "learning_rate": 6.648865153538051e-06,
      "loss": 0.6035,
      "step": 15000
    },
    {
      "epoch": 4.688473520249222,
      "grad_norm": 17.21211814880371,
      "learning_rate": 6.604361370716511e-06,
      "loss": 0.6521,
      "step": 15050
    },
    {
      "epoch": 4.70404984423676,
      "grad_norm": 23.095712661743164,
      "learning_rate": 6.559857587894971e-06,
      "loss": 0.6449,
      "step": 15100
    },
    {
      "epoch": 4.719626168224299,
      "grad_norm": 19.2949161529541,
      "learning_rate": 6.515353805073432e-06,
      "loss": 0.6771,
      "step": 15150
    },
    {
      "epoch": 4.735202492211838,
      "grad_norm": 17.471534729003906,
      "learning_rate": 6.470850022251892e-06,
      "loss": 0.6238,
      "step": 15200
    },
    {
      "epoch": 4.750778816199377,
      "grad_norm": 34.614227294921875,
      "learning_rate": 6.426346239430352e-06,
      "loss": 0.7662,
      "step": 15250
    },
    {
      "epoch": 4.766355140186916,
      "grad_norm": 17.420068740844727,
      "learning_rate": 6.381842456608812e-06,
      "loss": 0.7022,
      "step": 15300
    },
    {
      "epoch": 4.781931464174455,
      "grad_norm": 19.81572151184082,
      "learning_rate": 6.3373386737872725e-06,
      "loss": 0.6352,
      "step": 15350
    },
    {
      "epoch": 4.797507788161994,
      "grad_norm": 26.398540496826172,
      "learning_rate": 6.292834890965732e-06,
      "loss": 0.7197,
      "step": 15400
    },
    {
      "epoch": 4.813084112149532,
      "grad_norm": 26.638507843017578,
      "learning_rate": 6.2483311081441935e-06,
      "loss": 0.6163,
      "step": 15450
    },
    {
      "epoch": 4.8286604361370715,
      "grad_norm": 21.718738555908203,
      "learning_rate": 6.203827325322653e-06,
      "loss": 0.678,
      "step": 15500
    },
    {
      "epoch": 4.844236760124611,
      "grad_norm": 17.132883071899414,
      "learning_rate": 6.159323542501114e-06,
      "loss": 0.5538,
      "step": 15550
    },
    {
      "epoch": 4.859813084112149,
      "grad_norm": 16.80150032043457,
      "learning_rate": 6.114819759679573e-06,
      "loss": 0.6127,
      "step": 15600
    },
    {
      "epoch": 4.8753894080996885,
      "grad_norm": 25.9669189453125,
      "learning_rate": 6.070315976858033e-06,
      "loss": 0.6849,
      "step": 15650
    },
    {
      "epoch": 4.890965732087228,
      "grad_norm": 22.0374813079834,
      "learning_rate": 6.0258121940364935e-06,
      "loss": 0.6737,
      "step": 15700
    },
    {
      "epoch": 4.906542056074766,
      "grad_norm": 27.15550422668457,
      "learning_rate": 5.981308411214953e-06,
      "loss": 0.6977,
      "step": 15750
    },
    {
      "epoch": 4.922118380062305,
      "grad_norm": 11.834212303161621,
      "learning_rate": 5.9368046283934145e-06,
      "loss": 0.6391,
      "step": 15800
    },
    {
      "epoch": 4.937694704049845,
      "grad_norm": 15.391887664794922,
      "learning_rate": 5.892300845571874e-06,
      "loss": 0.5607,
      "step": 15850
    },
    {
      "epoch": 4.953271028037383,
      "grad_norm": 19.295982360839844,
      "learning_rate": 5.847797062750335e-06,
      "loss": 0.6797,
      "step": 15900
    },
    {
      "epoch": 4.968847352024922,
      "grad_norm": 11.136104583740234,
      "learning_rate": 5.803293279928794e-06,
      "loss": 0.6112,
      "step": 15950
    },
    {
      "epoch": 4.984423676012461,
      "grad_norm": 19.703954696655273,
      "learning_rate": 5.758789497107255e-06,
      "loss": 0.601,
      "step": 16000
    },
    {
      "epoch": 5.0,
      "grad_norm": 31.599714279174805,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 0.6512,
      "step": 16050
    },
    {
      "epoch": 5.0,
      "eval_accuracy": 0.7567167666069621,
      "eval_f1_score": 0.7543488610705766,
      "eval_loss": 1.0331050157546997,
      "eval_runtime": 45.7423,
      "eval_samples_per_second": 280.725,
      "eval_steps_per_second": 17.555,
      "step": 16050
    },
    {
      "epoch": 5.015576323987539,
      "grad_norm": 12.415169715881348,
      "learning_rate": 5.669781931464174e-06,
      "loss": 0.5524,
      "step": 16100
    },
    {
      "epoch": 5.031152647975078,
      "grad_norm": 34.26673889160156,
      "learning_rate": 5.6252781486426355e-06,
      "loss": 0.5771,
      "step": 16150
    },
    {
      "epoch": 5.046728971962617,
      "grad_norm": 12.439640998840332,
      "learning_rate": 5.580774365821095e-06,
      "loss": 0.5408,
      "step": 16200
    },
    {
      "epoch": 5.062305295950155,
      "grad_norm": 19.603832244873047,
      "learning_rate": 5.536270582999556e-06,
      "loss": 0.5537,
      "step": 16250
    },
    {
      "epoch": 5.077881619937695,
      "grad_norm": 27.543062210083008,
      "learning_rate": 5.491766800178015e-06,
      "loss": 0.5215,
      "step": 16300
    },
    {
      "epoch": 5.093457943925234,
      "grad_norm": 22.542049407958984,
      "learning_rate": 5.447263017356476e-06,
      "loss": 0.4901,
      "step": 16350
    },
    {
      "epoch": 5.109034267912772,
      "grad_norm": 12.443504333496094,
      "learning_rate": 5.4027592345349355e-06,
      "loss": 0.4649,
      "step": 16400
    },
    {
      "epoch": 5.1246105919003115,
      "grad_norm": 16.88738441467285,
      "learning_rate": 5.358255451713395e-06,
      "loss": 0.6318,
      "step": 16450
    },
    {
      "epoch": 5.140186915887851,
      "grad_norm": 24.19925308227539,
      "learning_rate": 5.3137516688918565e-06,
      "loss": 0.5199,
      "step": 16500
    },
    {
      "epoch": 5.155763239875389,
      "grad_norm": 17.895509719848633,
      "learning_rate": 5.269247886070317e-06,
      "loss": 0.5427,
      "step": 16550
    },
    {
      "epoch": 5.1713395638629285,
      "grad_norm": 20.32611846923828,
      "learning_rate": 5.224744103248777e-06,
      "loss": 0.5235,
      "step": 16600
    },
    {
      "epoch": 5.186915887850467,
      "grad_norm": 17.70724105834961,
      "learning_rate": 5.180240320427236e-06,
      "loss": 0.5191,
      "step": 16650
    },
    {
      "epoch": 5.202492211838006,
      "grad_norm": 24.63974380493164,
      "learning_rate": 5.135736537605697e-06,
      "loss": 0.4894,
      "step": 16700
    },
    {
      "epoch": 5.218068535825545,
      "grad_norm": 14.489203453063965,
      "learning_rate": 5.0912327547841565e-06,
      "loss": 0.5537,
      "step": 16750
    },
    {
      "epoch": 5.233644859813084,
      "grad_norm": 21.299999237060547,
      "learning_rate": 5.046728971962617e-06,
      "loss": 0.5888,
      "step": 16800
    },
    {
      "epoch": 5.249221183800623,
      "grad_norm": 31.91375732421875,
      "learning_rate": 5.0022251891410775e-06,
      "loss": 0.5357,
      "step": 16850
    },
    {
      "epoch": 5.264797507788162,
      "grad_norm": 18.332225799560547,
      "learning_rate": 4.957721406319538e-06,
      "loss": 0.5673,
      "step": 16900
    },
    {
      "epoch": 5.280373831775701,
      "grad_norm": 25.386491775512695,
      "learning_rate": 4.913217623497998e-06,
      "loss": 0.5292,
      "step": 16950
    },
    {
      "epoch": 5.29595015576324,
      "grad_norm": 20.261445999145508,
      "learning_rate": 4.868713840676458e-06,
      "loss": 0.4936,
      "step": 17000
    },
    {
      "epoch": 5.311526479750778,
      "grad_norm": 34.79202651977539,
      "learning_rate": 4.824210057854918e-06,
      "loss": 0.5474,
      "step": 17050
    },
    {
      "epoch": 5.327102803738318,
      "grad_norm": 33.6504020690918,
      "learning_rate": 4.779706275033378e-06,
      "loss": 0.5042,
      "step": 17100
    },
    {
      "epoch": 5.342679127725857,
      "grad_norm": 12.717674255371094,
      "learning_rate": 4.735202492211838e-06,
      "loss": 0.5158,
      "step": 17150
    },
    {
      "epoch": 5.358255451713395,
      "grad_norm": 20.64293670654297,
      "learning_rate": 4.6906987093902985e-06,
      "loss": 0.5425,
      "step": 17200
    },
    {
      "epoch": 5.373831775700935,
      "grad_norm": 38.552494049072266,
      "learning_rate": 4.646194926568759e-06,
      "loss": 0.5817,
      "step": 17250
    },
    {
      "epoch": 5.389408099688474,
      "grad_norm": 27.999866485595703,
      "learning_rate": 4.601691143747219e-06,
      "loss": 0.5184,
      "step": 17300
    },
    {
      "epoch": 5.404984423676012,
      "grad_norm": 21.099876403808594,
      "learning_rate": 4.557187360925679e-06,
      "loss": 0.5786,
      "step": 17350
    },
    {
      "epoch": 5.420560747663552,
      "grad_norm": 20.052082061767578,
      "learning_rate": 4.51268357810414e-06,
      "loss": 0.5542,
      "step": 17400
    },
    {
      "epoch": 5.43613707165109,
      "grad_norm": 17.854543685913086,
      "learning_rate": 4.468179795282599e-06,
      "loss": 0.5052,
      "step": 17450
    },
    {
      "epoch": 5.451713395638629,
      "grad_norm": 35.17618942260742,
      "learning_rate": 4.42367601246106e-06,
      "loss": 0.495,
      "step": 17500
    },
    {
      "epoch": 5.4672897196261685,
      "grad_norm": 10.24816608428955,
      "learning_rate": 4.3791722296395195e-06,
      "loss": 0.5335,
      "step": 17550
    },
    {
      "epoch": 5.482866043613707,
      "grad_norm": 23.191564559936523,
      "learning_rate": 4.33466844681798e-06,
      "loss": 0.5133,
      "step": 17600
    },
    {
      "epoch": 5.498442367601246,
      "grad_norm": 26.919538497924805,
      "learning_rate": 4.29016466399644e-06,
      "loss": 0.5778,
      "step": 17650
    },
    {
      "epoch": 5.5140186915887845,
      "grad_norm": 12.932025909423828,
      "learning_rate": 4.2456608811749e-06,
      "loss": 0.5044,
      "step": 17700
    },
    {
      "epoch": 5.529595015576324,
      "grad_norm": 21.92264175415039,
      "learning_rate": 4.201157098353361e-06,
      "loss": 0.4447,
      "step": 17750
    },
    {
      "epoch": 5.545171339563863,
      "grad_norm": 22.575197219848633,
      "learning_rate": 4.15665331553182e-06,
      "loss": 0.5211,
      "step": 17800
    },
    {
      "epoch": 5.5607476635514015,
      "grad_norm": 32.434139251708984,
      "learning_rate": 4.112149532710281e-06,
      "loss": 0.5582,
      "step": 17850
    },
    {
      "epoch": 5.576323987538941,
      "grad_norm": 19.929428100585938,
      "learning_rate": 4.0676457498887405e-06,
      "loss": 0.5452,
      "step": 17900
    },
    {
      "epoch": 5.59190031152648,
      "grad_norm": 14.786026000976562,
      "learning_rate": 4.023141967067201e-06,
      "loss": 0.5428,
      "step": 17950
    },
    {
      "epoch": 5.607476635514018,
      "grad_norm": 34.760581970214844,
      "learning_rate": 3.9786381842456615e-06,
      "loss": 0.5223,
      "step": 18000
    },
    {
      "epoch": 5.623052959501558,
      "grad_norm": 14.79931640625,
      "learning_rate": 3.934134401424121e-06,
      "loss": 0.5085,
      "step": 18050
    },
    {
      "epoch": 5.638629283489097,
      "grad_norm": 16.961246490478516,
      "learning_rate": 3.889630618602582e-06,
      "loss": 0.5869,
      "step": 18100
    },
    {
      "epoch": 5.654205607476635,
      "grad_norm": 9.952116012573242,
      "learning_rate": 3.845126835781041e-06,
      "loss": 0.492,
      "step": 18150
    },
    {
      "epoch": 5.669781931464175,
      "grad_norm": 33.834716796875,
      "learning_rate": 3.800623052959502e-06,
      "loss": 0.5227,
      "step": 18200
    },
    {
      "epoch": 5.685358255451713,
      "grad_norm": 31.821529388427734,
      "learning_rate": 3.7561192701379623e-06,
      "loss": 0.5377,
      "step": 18250
    },
    {
      "epoch": 5.700934579439252,
      "grad_norm": 12.08321475982666,
      "learning_rate": 3.7116154873164224e-06,
      "loss": 0.4935,
      "step": 18300
    },
    {
      "epoch": 5.716510903426792,
      "grad_norm": 23.746417999267578,
      "learning_rate": 3.6671117044948825e-06,
      "loss": 0.4984,
      "step": 18350
    },
    {
      "epoch": 5.73208722741433,
      "grad_norm": 13.351593971252441,
      "learning_rate": 3.622607921673342e-06,
      "loss": 0.5407,
      "step": 18400
    },
    {
      "epoch": 5.747663551401869,
      "grad_norm": 21.685117721557617,
      "learning_rate": 3.5781041388518027e-06,
      "loss": 0.4751,
      "step": 18450
    },
    {
      "epoch": 5.763239875389408,
      "grad_norm": 27.539573669433594,
      "learning_rate": 3.5336003560302627e-06,
      "loss": 0.5494,
      "step": 18500
    },
    {
      "epoch": 5.778816199376947,
      "grad_norm": 30.421785354614258,
      "learning_rate": 3.489096573208723e-06,
      "loss": 0.6028,
      "step": 18550
    },
    {
      "epoch": 5.794392523364486,
      "grad_norm": 16.4693546295166,
      "learning_rate": 3.4445927903871833e-06,
      "loss": 0.5568,
      "step": 18600
    },
    {
      "epoch": 5.809968847352025,
      "grad_norm": 21.82815170288086,
      "learning_rate": 3.4000890075656434e-06,
      "loss": 0.4645,
      "step": 18650
    },
    {
      "epoch": 5.825545171339564,
      "grad_norm": 29.782384872436523,
      "learning_rate": 3.3555852247441035e-06,
      "loss": 0.6048,
      "step": 18700
    },
    {
      "epoch": 5.841121495327103,
      "grad_norm": 11.159719467163086,
      "learning_rate": 3.3110814419225636e-06,
      "loss": 0.5452,
      "step": 18750
    },
    {
      "epoch": 5.8566978193146415,
      "grad_norm": 18.232776641845703,
      "learning_rate": 3.266577659101024e-06,
      "loss": 0.5158,
      "step": 18800
    },
    {
      "epoch": 5.872274143302181,
      "grad_norm": 24.63319969177246,
      "learning_rate": 3.222073876279484e-06,
      "loss": 0.5336,
      "step": 18850
    },
    {
      "epoch": 5.88785046728972,
      "grad_norm": 14.262673377990723,
      "learning_rate": 3.177570093457944e-06,
      "loss": 0.4657,
      "step": 18900
    },
    {
      "epoch": 5.9034267912772584,
      "grad_norm": 15.649080276489258,
      "learning_rate": 3.1330663106364047e-06,
      "loss": 0.5891,
      "step": 18950
    },
    {
      "epoch": 5.919003115264798,
      "grad_norm": 27.676998138427734,
      "learning_rate": 3.0885625278148644e-06,
      "loss": 0.5213,
      "step": 19000
    },
    {
      "epoch": 5.934579439252336,
      "grad_norm": 26.382436752319336,
      "learning_rate": 3.0440587449933245e-06,
      "loss": 0.5799,
      "step": 19050
    },
    {
      "epoch": 5.950155763239875,
      "grad_norm": 18.606721878051758,
      "learning_rate": 2.999554962171785e-06,
      "loss": 0.5684,
      "step": 19100
    },
    {
      "epoch": 5.965732087227415,
      "grad_norm": 24.351234436035156,
      "learning_rate": 2.955051179350245e-06,
      "loss": 0.4887,
      "step": 19150
    },
    {
      "epoch": 5.981308411214953,
      "grad_norm": 15.265799522399902,
      "learning_rate": 2.910547396528705e-06,
      "loss": 0.5207,
      "step": 19200
    },
    {
      "epoch": 5.996884735202492,
      "grad_norm": 19.805349349975586,
      "learning_rate": 2.8660436137071652e-06,
      "loss": 0.5343,
      "step": 19250
    },
    {
      "epoch": 6.0,
      "eval_accuracy": 0.7727591309088078,
      "eval_f1_score": 0.7717075325658431,
      "eval_loss": 0.9984089732170105,
      "eval_runtime": 45.6325,
      "eval_samples_per_second": 281.4,
      "eval_steps_per_second": 17.597,
      "step": 19260
    },
    {
      "epoch": 6.012461059190031,
      "grad_norm": 29.618614196777344,
      "learning_rate": 2.8215398308856257e-06,
      "loss": 0.527,
      "step": 19300
    },
    {
      "epoch": 6.02803738317757,
      "grad_norm": 28.5826416015625,
      "learning_rate": 2.777036048064086e-06,
      "loss": 0.4548,
      "step": 19350
    },
    {
      "epoch": 6.043613707165109,
      "grad_norm": 13.652647018432617,
      "learning_rate": 2.7325322652425455e-06,
      "loss": 0.4156,
      "step": 19400
    },
    {
      "epoch": 6.059190031152648,
      "grad_norm": 31.47962760925293,
      "learning_rate": 2.6880284824210064e-06,
      "loss": 0.4964,
      "step": 19450
    },
    {
      "epoch": 6.074766355140187,
      "grad_norm": 28.731477737426758,
      "learning_rate": 2.643524699599466e-06,
      "loss": 0.5524,
      "step": 19500
    },
    {
      "epoch": 6.090342679127726,
      "grad_norm": 16.10720443725586,
      "learning_rate": 2.599020916777926e-06,
      "loss": 0.4671,
      "step": 19550
    },
    {
      "epoch": 6.105919003115265,
      "grad_norm": 30.433786392211914,
      "learning_rate": 2.5545171339563862e-06,
      "loss": 0.448,
      "step": 19600
    },
    {
      "epoch": 6.121495327102804,
      "grad_norm": 20.700838088989258,
      "learning_rate": 2.5100133511348467e-06,
      "loss": 0.4337,
      "step": 19650
    },
    {
      "epoch": 6.137071651090342,
      "grad_norm": 13.195507049560547,
      "learning_rate": 2.465509568313307e-06,
      "loss": 0.4064,
      "step": 19700
    },
    {
      "epoch": 6.1526479750778815,
      "grad_norm": 14.922452926635742,
      "learning_rate": 2.421005785491767e-06,
      "loss": 0.4093,
      "step": 19750
    },
    {
      "epoch": 6.168224299065421,
      "grad_norm": 10.15404987335205,
      "learning_rate": 2.376502002670227e-06,
      "loss": 0.4505,
      "step": 19800
    },
    {
      "epoch": 6.183800623052959,
      "grad_norm": 7.500768661499023,
      "learning_rate": 2.3319982198486875e-06,
      "loss": 0.4866,
      "step": 19850
    },
    {
      "epoch": 6.1993769470404985,
      "grad_norm": 11.353964805603027,
      "learning_rate": 2.2874944370271476e-06,
      "loss": 0.4555,
      "step": 19900
    },
    {
      "epoch": 6.214953271028038,
      "grad_norm": 21.45136070251465,
      "learning_rate": 2.2429906542056077e-06,
      "loss": 0.4804,
      "step": 19950
    },
    {
      "epoch": 6.230529595015576,
      "grad_norm": 12.098871231079102,
      "learning_rate": 2.1984868713840677e-06,
      "loss": 0.4188,
      "step": 20000
    },
    {
      "epoch": 6.246105919003115,
      "grad_norm": 23.454179763793945,
      "learning_rate": 2.153983088562528e-06,
      "loss": 0.4643,
      "step": 20050
    },
    {
      "epoch": 6.261682242990654,
      "grad_norm": 13.97701358795166,
      "learning_rate": 2.1094793057409883e-06,
      "loss": 0.4902,
      "step": 20100
    },
    {
      "epoch": 6.277258566978193,
      "grad_norm": 11.761476516723633,
      "learning_rate": 2.0649755229194484e-06,
      "loss": 0.4092,
      "step": 20150
    },
    {
      "epoch": 6.292834890965732,
      "grad_norm": 22.017658233642578,
      "learning_rate": 2.0204717400979085e-06,
      "loss": 0.4982,
      "step": 20200
    },
    {
      "epoch": 6.308411214953271,
      "grad_norm": 10.849244117736816,
      "learning_rate": 1.9759679572763686e-06,
      "loss": 0.4472,
      "step": 20250
    },
    {
      "epoch": 6.32398753894081,
      "grad_norm": 14.393492698669434,
      "learning_rate": 1.9314641744548286e-06,
      "loss": 0.431,
      "step": 20300
    },
    {
      "epoch": 6.339563862928349,
      "grad_norm": 18.863243103027344,
      "learning_rate": 1.886960391633289e-06,
      "loss": 0.4991,
      "step": 20350
    },
    {
      "epoch": 6.355140186915888,
      "grad_norm": 13.154719352722168,
      "learning_rate": 1.842456608811749e-06,
      "loss": 0.4884,
      "step": 20400
    },
    {
      "epoch": 6.370716510903427,
      "grad_norm": 24.44037628173828,
      "learning_rate": 1.7979528259902093e-06,
      "loss": 0.4901,
      "step": 20450
    },
    {
      "epoch": 6.386292834890965,
      "grad_norm": 13.993340492248535,
      "learning_rate": 1.7534490431686696e-06,
      "loss": 0.469,
      "step": 20500
    },
    {
      "epoch": 6.401869158878505,
      "grad_norm": 9.250417709350586,
      "learning_rate": 1.7089452603471297e-06,
      "loss": 0.4828,
      "step": 20550
    },
    {
      "epoch": 6.417445482866044,
      "grad_norm": 36.1077766418457,
      "learning_rate": 1.6644414775255898e-06,
      "loss": 0.4653,
      "step": 20600
    },
    {
      "epoch": 6.433021806853582,
      "grad_norm": 21.920188903808594,
      "learning_rate": 1.6199376947040499e-06,
      "loss": 0.4905,
      "step": 20650
    },
    {
      "epoch": 6.4485981308411215,
      "grad_norm": 21.98137855529785,
      "learning_rate": 1.5754339118825101e-06,
      "loss": 0.5067,
      "step": 20700
    },
    {
      "epoch": 6.464174454828661,
      "grad_norm": 12.723950386047363,
      "learning_rate": 1.5309301290609704e-06,
      "loss": 0.4287,
      "step": 20750
    },
    {
      "epoch": 6.479750778816199,
      "grad_norm": 5.50515604019165,
      "learning_rate": 1.4864263462394305e-06,
      "loss": 0.4673,
      "step": 20800
    },
    {
      "epoch": 6.4953271028037385,
      "grad_norm": 16.74237632751465,
      "learning_rate": 1.4419225634178908e-06,
      "loss": 0.4383,
      "step": 20850
    },
    {
      "epoch": 6.510903426791277,
      "grad_norm": 21.08042335510254,
      "learning_rate": 1.3974187805963507e-06,
      "loss": 0.4845,
      "step": 20900
    },
    {
      "epoch": 6.526479750778816,
      "grad_norm": 9.89013385772705,
      "learning_rate": 1.352914997774811e-06,
      "loss": 0.4555,
      "step": 20950
    },
    {
      "epoch": 6.542056074766355,
      "grad_norm": 18.200185775756836,
      "learning_rate": 1.308411214953271e-06,
      "loss": 0.4887,
      "step": 21000
    },
    {
      "epoch": 6.557632398753894,
      "grad_norm": 17.719005584716797,
      "learning_rate": 1.2639074321317314e-06,
      "loss": 0.4496,
      "step": 21050
    },
    {
      "epoch": 6.573208722741433,
      "grad_norm": 25.08338165283203,
      "learning_rate": 1.2194036493101914e-06,
      "loss": 0.4729,
      "step": 21100
    },
    {
      "epoch": 6.588785046728972,
      "grad_norm": 10.096006393432617,
      "learning_rate": 1.1748998664886515e-06,
      "loss": 0.3815,
      "step": 21150
    },
    {
      "epoch": 6.604361370716511,
      "grad_norm": 24.757503509521484,
      "learning_rate": 1.1303960836671118e-06,
      "loss": 0.4233,
      "step": 21200
    },
    {
      "epoch": 6.61993769470405,
      "grad_norm": 23.61713981628418,
      "learning_rate": 1.0858923008455719e-06,
      "loss": 0.4709,
      "step": 21250
    },
    {
      "epoch": 6.635514018691588,
      "grad_norm": 18.363285064697266,
      "learning_rate": 1.0413885180240322e-06,
      "loss": 0.5356,
      "step": 21300
    },
    {
      "epoch": 6.651090342679128,
      "grad_norm": 22.178518295288086,
      "learning_rate": 9.968847352024923e-07,
      "loss": 0.503,
      "step": 21350
    },
    {
      "epoch": 6.666666666666667,
      "grad_norm": 18.805871963500977,
      "learning_rate": 9.523809523809525e-07,
      "loss": 0.405,
      "step": 21400
    },
    {
      "epoch": 6.682242990654205,
      "grad_norm": 16.61947250366211,
      "learning_rate": 9.078771695594126e-07,
      "loss": 0.4134,
      "step": 21450
    },
    {
      "epoch": 6.697819314641745,
      "grad_norm": 25.02368927001953,
      "learning_rate": 8.633733867378728e-07,
      "loss": 0.4889,
      "step": 21500
    },
    {
      "epoch": 6.713395638629283,
      "grad_norm": 30.051013946533203,
      "learning_rate": 8.18869603916333e-07,
      "loss": 0.3979,
      "step": 21550
    },
    {
      "epoch": 6.728971962616822,
      "grad_norm": 17.057714462280273,
      "learning_rate": 7.743658210947931e-07,
      "loss": 0.4232,
      "step": 21600
    },
    {
      "epoch": 6.744548286604362,
      "grad_norm": 13.834571838378906,
      "learning_rate": 7.298620382732533e-07,
      "loss": 0.4423,
      "step": 21650
    },
    {
      "epoch": 6.7601246105919,
      "grad_norm": 16.11556625366211,
      "learning_rate": 6.853582554517134e-07,
      "loss": 0.4982,
      "step": 21700
    },
    {
      "epoch": 6.775700934579439,
      "grad_norm": 35.68506622314453,
      "learning_rate": 6.408544726301737e-07,
      "loss": 0.4581,
      "step": 21750
    },
    {
      "epoch": 6.7912772585669785,
      "grad_norm": 10.286312103271484,
      "learning_rate": 5.963506898086338e-07,
      "loss": 0.4529,
      "step": 21800
    },
    {
      "epoch": 6.806853582554517,
      "grad_norm": 25.99221420288086,
      "learning_rate": 5.518469069870939e-07,
      "loss": 0.492,
      "step": 21850
    },
    {
      "epoch": 6.822429906542056,
      "grad_norm": 27.993797302246094,
      "learning_rate": 5.073431241655541e-07,
      "loss": 0.4218,
      "step": 21900
    },
    {
      "epoch": 6.838006230529595,
      "grad_norm": 15.832100868225098,
      "learning_rate": 4.628393413440143e-07,
      "loss": 0.4871,
      "step": 21950
    },
    {
      "epoch": 6.853582554517134,
      "grad_norm": 23.259050369262695,
      "learning_rate": 4.1833555852247444e-07,
      "loss": 0.4891,
      "step": 22000
    },
    {
      "epoch": 6.869158878504673,
      "grad_norm": 30.341997146606445,
      "learning_rate": 3.7383177570093457e-07,
      "loss": 0.3882,
      "step": 22050
    },
    {
      "epoch": 6.8847352024922115,
      "grad_norm": 21.55188751220703,
      "learning_rate": 3.293279928793948e-07,
      "loss": 0.4412,
      "step": 22100
    },
    {
      "epoch": 6.900311526479751,
      "grad_norm": 20.833377838134766,
      "learning_rate": 2.8482421005785495e-07,
      "loss": 0.4078,
      "step": 22150
    },
    {
      "epoch": 6.91588785046729,
      "grad_norm": 16.139249801635742,
      "learning_rate": 2.403204272363151e-07,
      "loss": 0.458,
      "step": 22200
    },
    {
      "epoch": 6.931464174454828,
      "grad_norm": 14.878101348876953,
      "learning_rate": 1.958166444147753e-07,
      "loss": 0.5384,
      "step": 22250
    },
    {
      "epoch": 6.947040498442368,
      "grad_norm": 16.175329208374023,
      "learning_rate": 1.5131286159323543e-07,
      "loss": 0.4573,
      "step": 22300
    },
    {
      "epoch": 6.962616822429906,
      "grad_norm": 17.5386962890625,
      "learning_rate": 1.068090787716956e-07,
      "loss": 0.441,
      "step": 22350
    },
    {
      "epoch": 6.978193146417445,
      "grad_norm": 23.78289031982422,
      "learning_rate": 6.230529595015577e-08,
      "loss": 0.4576,
      "step": 22400
    },
    {
      "epoch": 6.993769470404985,
      "grad_norm": 28.538162231445312,
      "learning_rate": 1.7801513128615934e-08,
      "loss": 0.4505,
      "step": 22450
    },
    {
      "epoch": 7.0,
      "eval_accuracy": 0.778288295304104,
      "eval_f1_score": 0.7770437583839159,
      "eval_loss": 0.9968264102935791,
      "eval_runtime": 45.8288,
      "eval_samples_per_second": 280.195,
      "eval_steps_per_second": 17.522,
      "step": 22470
    }
  ],
  "logging_steps": 50,
  "max_steps": 22470,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 7,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.191763559153664e+16,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
